# CNN image encoder
import torch
import torchvision


class cnnEncoder(torch.nn.Module):
    def __init__(self):
        super().__init__()

        # number of features in a text representation of the caption generated by the bilstm encoder
        self.numFeatures = 256

        # cnn encoder that the paper uses is based on inception_v3
        pretrainedInceptionV3 = torchvision.models.inception_v3(pretrained=True, transform_input=False)

        # ensure that core weights do not change
        for parameter in pretrainedInceptionV3.parameters():
            # no updates
            parameter.requires_grad = False
        self.inceptionUpToLocalFeatures, self.restOfInception = self.pickModulesFromInception(pretrainedInceptionV3)

        # defining additional layers that the author adds for processing local and global features
        # # extracted from inception v3

        # default stride is 1 and padding is 0 and bias is on by default also
        self.embedLocalFeatures = torch.nn.Conv2d(768,self.numFeatures,kernel_size=1)
        self.embedGlobalFeatures = torch.nn.Linear(2048,self.numFeatures)

        # initializing weights for trainable (non-inception) parts
        self.initWeights()

    def forward(self, x):
        # input of batch x 3 x 299 x 299
        x = torch.nn.functional.interpolate(x, size=(299,299), mode='bilinear', align_corners=False)
        # processing input through inception
        x = self.inceptionUpToLocalFeatures(x)
        # extracting local features
        localFeatures = x
        # running rest of inception to get global features
        x = self.restOfInception(x)

        # flattening x since it has shape (1,1,2048) and we need (1,2048) to send to the linear layer
        x = torch.flatten(x,start_dim=1)

        # embedding local features
        localFeatures = self.embedLocalFeatures(localFeatures)

        # embedding global features
        globalFeatures = self.embedGlobalFeatures(x)

        return localFeatures, globalFeatures



    # grabbing layers of inception that we need
    def pickModulesFromInception(self, inceptionv3):
        # list of modules from inception v3 up to the mixed 6e layer, after which the authors extract local features
        modulesUpToLocalFeatures = torch.nn.Sequential(
            inceptionv3.Conv2d_1a_3x3,
            inceptionv3.Conv2d_2a_3x3,
            inceptionv3.Conv2d_2b_3x3,
            inceptionv3.maxpool1,
            inceptionv3.Conv2d_3b_1x1,
            inceptionv3.Conv2d_4a_3x3,
            inceptionv3.maxpool2,
            inceptionv3.Mixed_5b,
            inceptionv3.Mixed_5c,
            inceptionv3.Mixed_5d,
            inceptionv3.Mixed_6a,
            inceptionv3.Mixed_6b,
            inceptionv3.Mixed_6c,
            inceptionv3.Mixed_6d,
            inceptionv3.Mixed_6e
        )
        restOfInceptionModules = torch.nn.Sequential(
            inceptionv3.Mixed_7a,
            inceptionv3.Mixed_7b,
            inceptionv3.Mixed_7c,
            inceptionv3.avgpool
        )
        return modulesUpToLocalFeatures, restOfInceptionModules

    # initializing weights of the last layers (layers that the authors add that are not part of inception v3)
    def initWeights(self):
        # authors initialize uniformly from -0.1 to 0.1
        self.embedLocalFeatures.weight.data.uniform_(-0.1,0.1)
        self.embedGlobalFeatures.weight.data.uniform_(-0.1,0.1)



